# GPUåŠ é€Ÿæ•°æ®å¤„ç†å™¨
# ä½¿ç”¨GPUå¤§å¹…æå‡BERTç‰¹å¾æå–é€Ÿåº¦

import pandas as pd
import numpy as np
import pickle
import torch
import jieba
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm
import re
from collections import defaultdict, Counter
import random
import os
import gc
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['TRANSFORMERS_DISABLE_AUDIO_PROCESSING'] = '1'

class GPUProcessor:
    def __init__(self, model_name='hfl/chinese-roberta-wwm-ext', batch_size=64):
        """
        GPUåŠ é€Ÿæ•°æ®å¤„ç†å™¨
        
        Args:
            model_name: RoBERTaæ¨¡å‹åç§°
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆGPUå¯ä»¥å¤„ç†æ›´å¤§çš„æ‰¹æ¬¡ï¼‰
        """
        self.model_name = model_name
        self.batch_size = batch_size
        
        print(f"æ­£åœ¨åŠ è½½RoBERTaæ¨¡å‹: {model_name}")
        print(f"æ‰¹å¤„ç†å¤§å°: {batch_size}")
        
        # æ£€æŸ¥GPUå¯ç”¨æ€§
        if torch.cuda.is_available():
            self.device = torch.device('cuda')
            print(f"âœ… æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name(0)}")
            print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        else:
            self.device = torch.device('cpu')
            print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
        
        # åŠ è½½æ¨¡å‹ - åªä½¿ç”¨chinese-roberta-wwm-extï¼Œä¸ä½¿ç”¨å¤‡ç”¨æ¨¡å‹
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
        print("æ³¨æ„: åªä½¿ç”¨chinese-roberta-wwm-extæ¨¡å‹ï¼Œä¸ä½¿ç”¨å¤‡ç”¨æ¨¡å‹")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModel.from_pretrained(model_name)
            print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ: {type(self.model).__name__}")
            print(f"âœ“ æ¨¡å‹åç§°: {model_name}")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("è¯·ç¡®ä¿:")
            print("1. ç½‘ç»œè¿æ¥æ­£å¸¸")
            print("2. æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´")
            print("3. transformersåº“ç‰ˆæœ¬æ­£ç¡®")
            print("4. æ¨¡å‹åç§°æ­£ç¡®: hfl/chinese-roberta-wwm-ext")
            raise RuntimeError(f"æ— æ³•åŠ è½½chinese-roberta-wwm-extæ¨¡å‹: {e}")
        
        self.model.to(self.device)
        self.model.eval()
        print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æƒ…ç»ªæ ‡ç­¾æ˜ å°„ï¼ˆæ”¯æŒå¤šç§æ•°æ®é›†ï¼‰
        self.emotion_map = {
            # æ˜ å°„ä¸ºneutral (0)
            'neutral': 0,
            'astonished': 0,  # surpriseæ˜ å°„ä¸ºneutral
            'surprised': 0,   # æ·»åŠ ï¼šconversationsæ•°æ®é›†
            
            # æ˜ å°„ä¸ºhappy (1)
            'relaxed': 1,
            'happy': 1,
            'grateful': 1,
            'positive-other': 1,
            
            # æ˜ å°„ä¸ºsad (2)
            'depress': 2,
            'fear': 2,
            'fearful': 2,     # æ·»åŠ ï¼šconversationsæ•°æ®é›†
            'negative-other': 2,
            'sadness': 2,
            'sad': 2,         # æ·»åŠ ï¼šconversationsæ•°æ®é›†
            'worried': 2,
            
            # æ˜ å°„ä¸ºangry (3)
            'anger': 3,
            'angry': 3,       # æ·»åŠ ï¼šconversationsæ•°æ®é›†
            'disgust': 3,
            'disgusted': 3,   # æ·»åŠ ï¼šconversationsæ•°æ®é›†
        }
        
        # è¯´è¯è€…æ˜ å°„
        self.speaker_map = {}
        self.speaker_counter = 0
        
    def clean_text(self, text):
        """å¿«é€Ÿæ¸…æ´—æ–‡æœ¬"""
        if pd.isna(text) or text == '':
            return ''
        
        text = str(text)
        # åªå»é™¤æ˜æ˜¾çš„ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™æ›´å¤šå†…å®¹
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', '', text)
        text = ' '.join(text.split())
        return text
    
    def get_speaker_id(self, speaker_name):
        """è·å–è¯´è¯è€…ID"""
        if speaker_name not in self.speaker_map:
            speaker_id = chr(ord('A') + self.speaker_counter)
            self.speaker_map[speaker_name] = speaker_id
            self.speaker_counter += 1
        return self.speaker_map[speaker_name]
    
    def extract_roberta_features_gpu(self, texts):
        """
        GPUåŠ é€Ÿæ‰¹é‡æå–RoBERTaç‰¹å¾
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
        
        Returns:
            list: ç‰¹å¾å‘é‡åˆ—è¡¨
        """
        if not texts:
            return []
        
        features = []
        
        # ä½¿ç”¨GPUæ‰¹å¤„ç†
        for i in range(0, len(texts), self.batch_size):
            batch_texts = texts[i:i + self.batch_size]
            
            try:
                # ç¼–ç 
                inputs = self.tokenizer(
                    batch_texts,
                    return_tensors='pt',
                    padding=True,
                    truncation=True,
                    max_length=128  # GPUå¯ä»¥å¤„ç†æ›´é•¿çš„åºåˆ—
                )
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                # æå–ç‰¹å¾
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    batch_features = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                
                features.extend(batch_features)
                
                # æ¸…ç†GPUå†…å­˜
                del inputs, outputs, batch_features
                torch.cuda.empty_cache()
                
            except RuntimeError as e:
                if "out of memory" in str(e):
                    print(f"GPUå†…å­˜ä¸è¶³ï¼Œå‡å°æ‰¹å¤„ç†å¤§å°...")
                    # å‡å°æ‰¹å¤„ç†å¤§å°é‡è¯•
                    smaller_batch_size = self.batch_size // 2
                    for j in range(0, len(batch_texts), smaller_batch_size):
                        small_batch = batch_texts[j:j + smaller_batch_size]
                        try:
                            small_inputs = self.tokenizer(
                                small_batch,
                                return_tensors='pt',
                                padding=True,
                                truncation=True,
                                max_length=128
                            )
                            small_inputs = {k: v.to(self.device) for k, v in small_inputs.items()}
                            
                            with torch.no_grad():
                                small_outputs = self.model(**small_inputs)
                                small_features = small_outputs.last_hidden_state[:, 0, :].cpu().numpy()
                            
                            features.extend(small_features)
                            
                            del small_inputs, small_outputs, small_features
                            torch.cuda.empty_cache()
                            
                        except Exception as e2:
                            print(f"å°æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e2}")
                            # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œä½¿ç”¨é›¶å‘é‡å¡«å……
                            features.extend([np.zeros(768) for _ in small_batch])
                else:
                    print(f"æ‰¹å¤„ç†ç‰¹å¾æå–å¤±è´¥: {e}")
                    # å›é€€åˆ°å•ä¸ªå¤„ç†
                    for text in batch_texts:
                        if text and text.strip():
                            try:
                                single_inputs = self.tokenizer(
                                    text,
                                    return_tensors='pt',
                                    padding=True,
                                    truncation=True,
                                    max_length=128
                                )
                                single_inputs = {k: v.to(self.device) for k, v in single_inputs.items()}
                                
                                with torch.no_grad():
                                    single_outputs = self.model(**single_inputs)
                                    single_feature = single_outputs.last_hidden_state[:, 0, :].cpu().numpy()
                                
                                features.append(single_feature[0])
                                
                                del single_inputs, single_outputs, single_feature
                                torch.cuda.empty_cache()
                                
                            except Exception as e2:
                                print(f"å•ä¸ªç‰¹å¾æå–å¤±è´¥: {text[:50]}... é”™è¯¯: {e2}")
                                features.append(np.zeros(768))
                        else:
                            features.append(np.zeros(768))
        
        return features
    
    def load_cped_data(self, file_path):
        """åŠ è½½CPEDæ•°æ®é›†"""
        print(f"æ­£åœ¨åŠ è½½CPEDæ•°æ®: {file_path}")
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # åˆ†å—è¯»å–å¤§æ–‡ä»¶
        chunk_size = 20000  # GPUå¯ä»¥å¤„ç†æ›´å¤§çš„å—
        chunks = []
        
        print("æ­£åœ¨åˆ†å—è¯»å–æ•°æ®...")
        for chunk in pd.read_csv(file_path, encoding='utf-8', chunksize=chunk_size):
            chunks.append(chunk)
        
        df = pd.concat(chunks, ignore_index=True)
        print(f"æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(df)} æ¡è®°å½•")
        print(f"æ•°æ®åˆ—: {list(df.columns)}")
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—
        required_columns = ['TV_ID', 'Utterance', 'Speaker', 'Emotion']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}")
        
        return df
    
    def group_by_conversation(self, df):
        """æŒ‰å¯¹è¯IDåˆ†ç»„æ•°æ®"""
        print("æ­£åœ¨æŒ‰å¯¹è¯IDåˆ†ç»„æ•°æ®...")
        conversations = defaultdict(list)
        
        for idx, row in df.iterrows():
            conv_id = row['TV_ID']
            conversations[conv_id].append(row)
        
        print(f"å…±æ‰¾åˆ° {len(conversations)} ä¸ªå¯¹è¯")
        
        # æ˜¾ç¤ºå¯¹è¯é•¿åº¦åˆ†å¸ƒ
        conv_lengths = [len(conv) for conv in conversations.values()]
        print(f"å¯¹è¯é•¿åº¦ç»Ÿè®¡: å¹³å‡ {np.mean(conv_lengths):.2f}, æœ€å° {min(conv_lengths)}, æœ€å¤§ {max(conv_lengths)}")
        
        return dict(conversations)
    
    def process_single_conversation(self, conv_id, utterances):
        """å¤„ç†å•ä¸ªå¯¹è¯"""
        # æŒ‰ç´¢å¼•é¡ºåºæ’åº
        utterances = sorted(utterances, key=lambda x: x.name)
        
        # æ”¶é›†æ‰€æœ‰æ–‡æœ¬
        texts = []
        speakers = []
        labels = []
        sentences = []
        
        for utterance in utterances:
            # æ¸…æ´—æ–‡æœ¬
            text = self.clean_text(utterance['Utterance'])
            if not text:
                continue
            
            # è·å–è¯´è¯è€…ID
            speaker_name = utterance['Speaker']
            speaker_id = self.get_speaker_id(speaker_name)
            
            # è·å–æƒ…ç»ªæ ‡ç­¾
            emotion = utterance['Emotion'].lower() if pd.notna(utterance['Emotion']) else 'neutral'
            emotion_id = self.emotion_map.get(emotion, 0)
            
            texts.append(text)
            speakers.append(speaker_id)
            labels.append(emotion_id)
            sentences.append(text)
        
        # æ£€æŸ¥å¯¹è¯æ˜¯å¦æœ‰æ•ˆ
        if len(texts) < 2 or len(set(speakers)) < 2:
            return None
        
        # GPUæ‰¹é‡æå–ç‰¹å¾
        print(f"æ­£åœ¨GPUæå–å¯¹è¯ {conv_id} çš„ç‰¹å¾...")
        text_features = self.extract_roberta_features_gpu(texts)
        
        return {
            'conv_id': conv_id,
            'speakers': speakers,
            'labels': labels,
            'text_features': text_features,
            'sentences': sentences,
            'speaker_count': len(set(speakers)),
            'turn_count': len(texts)
        }
    
    def convert_to_iemocap_format(self, conversations, output_file):
        """è½¬æ¢ä¸ºIEMOCAPæ ¼å¼ - åªä¿ç•™æ–‡æœ¬ç‰¹å¾"""
        print("æ­£åœ¨è½¬æ¢ä¸ºIEMOCAPæ ¼å¼ï¼ˆä»…æ–‡æœ¬ç‰¹å¾ï¼‰...")
        
        # åˆå§‹åŒ–æ•°æ®ç»“æ„ - åªä¿ç•™æ–‡æœ¬ç›¸å…³ç‰¹å¾
        video_ids = []
        video_speakers = {}
        video_labels = {}
        video_text = {}
        video_sentence = {}
        
        valid_conversations = 0
        
        for conv_id, conv_data in tqdm(conversations.items(), desc="è½¬æ¢å¯¹è¯"):
            if conv_data is None:
                continue
            
            video_ids.append(conv_id)
            video_speakers[conv_id] = conv_data['speakers']
            video_labels[conv_id] = conv_data['labels']
            video_text[conv_id] = conv_data['text_features']
            video_sentence[conv_id] = conv_data['sentences']
            
            valid_conversations += 1
        
        print(f"æœ‰æ•ˆå¯¹è¯æ•°: {valid_conversations}")
        
        # åˆ’åˆ†æ•°æ®é›†
        random.shuffle(video_ids)
        train_size = int(len(video_ids) * 0.7)
        dev_size = int(len(video_ids) * 0.15)
        
        trainVids = video_ids[:train_size]
        dev_vids = video_ids[train_size:train_size + dev_size]
        test_vids = video_ids[train_size + dev_size:]
        
        # # å¢å¼ºangry(3)å’Œsad(2)
        # video_ids, video_text, video_labels, video_speakers = self.augment_class(video_ids, video_text, video_labels, video_speakers, target_label=3, ratio=0.2)
        # video_ids, video_text, video_labels, video_speakers = self.augment_class(video_ids, video_text, video_labels, video_speakers, target_label=2, ratio=0.1)


        # è¿‡æ»¤æ‰æœ‰ç©ºå­—æ®µæˆ–å­—æ®µé•¿åº¦ä¸ä¸€è‡´çš„æ ·æœ¬
        filtered_ids = []
        for vid in video_ids:
            t = video_text.get(vid, [])
            l = video_labels.get(vid, [])
            s = video_speakers.get(vid, [])
            if t and l and s and len(t) == len(l) == len(s):
                filtered_ids.append(vid)
        video_ids = filtered_ids

        # ç±»åˆ«å‡è¡¡åˆ’åˆ†
        from collections import defaultdict
        label_dict = defaultdict(list)
        for vid in video_ids:
            label = video_labels.get(vid, [0])[0]
            label_dict[label].append(vid)
        trainVids, dev_vids, test_vids = [], [], []
        for vids in label_dict.values():
            random.shuffle(vids)
            n = len(vids)
            n_train = int(n * 0.7)
            n_dev = int(n * 0.15)
            trainVids.extend(vids[:n_train])
            dev_vids.extend(vids[n_train:n_train + n_dev])
            test_vids.extend(vids[n_train + n_dev:])
        random.shuffle(trainVids)
        random.shuffle(dev_vids)
        random.shuffle(test_vids)

        print(f"è®­ç»ƒé›†: {len(trainVids)} ä¸ªå¯¹è¯")
        print(f"éªŒè¯é›†: {len(dev_vids)} ä¸ªå¯¹è¯")
        print(f"æµ‹è¯•é›†: {len(test_vids)} ä¸ªå¯¹è¯")

        # ç»Ÿè®¡æƒ…ç»ªåˆ†å¸ƒ
        self.analyze_emotion_distribution_by_split(video_labels, trainVids, dev_vids, test_vids)

        # ä¿å­˜æ•°æ® - åªä¿å­˜æ–‡æœ¬ç›¸å…³ç‰¹å¾
        data = (video_ids, video_speakers, video_labels, video_text, trainVids, test_vids)
        with open(output_file, 'wb') as f:
            pickle.dump(data, f)
        print(f"æ•°æ®ä¿å­˜å®Œæˆ: {output_file}")
        
        # ä¿å­˜è¯´è¯è€…æ˜ å°„
        speaker_map_file = output_file.replace('.pkl', '_speaker_map.pkl')
        with open(speaker_map_file, 'wb') as f:
            pickle.dump(self.speaker_map, f)
        
        print(f"è¯´è¯è€…æ˜ å°„ä¿å­˜å®Œæˆ: {speaker_map_file}")
        
        return data
    
    def analyze_data_distribution(self, conversations):
        """åˆ†ææ•°æ®åˆ†å¸ƒ"""
        print("\n=== æ•°æ®åˆ†å¸ƒåˆ†æ ===")
        
        emotion_counts = defaultdict(int)
        speaker_counts = defaultdict(int)
        conversation_lengths = []
        speaker_counts_per_conv = []
        
        for conv_id, conv_data in conversations.items():
            if conv_data is None:
                continue
            
            conversation_lengths.append(conv_data['turn_count'])
            speaker_counts_per_conv.append(conv_data['speaker_count'])
            
            for i, speaker in enumerate(conv_data['speakers']):
                emotion_counts[conv_data['labels'][i]] += 1
                speaker_counts[speaker] += 1
        
        print(f"æƒ…ç»ªåˆ†å¸ƒ: {dict(emotion_counts)}")
        print(f"è¯´è¯è€…åˆ†å¸ƒ: {dict(speaker_counts)}")
        print(f"å¹³å‡å¯¹è¯é•¿åº¦: {np.mean(conversation_lengths):.2f}")
        print(f"å¹³å‡è¯´è¯è€…æ•°é‡: {np.mean(speaker_counts_per_conv):.2f}")
        print(f"æœ€å¤§å¯¹è¯é•¿åº¦: {max(conversation_lengths)}")
        print(f"æœ€å¤§è¯´è¯è€…æ•°é‡: {max(speaker_counts_per_conv)}")
    
    def analyze_emotion_distribution_by_split(self, video_labels, trainVids, dev_vids, test_vids):
        """åˆ†ææ•°æ®é›†åˆ’åˆ†åçš„æƒ…ç»ªåˆ†å¸ƒ"""
        print("\n" + "="*60)
        print("æ•°æ®é›†åˆ’åˆ†åæƒ…ç»ªåˆ†å¸ƒç»Ÿè®¡")
        print("="*60)
        
        # æƒ…ç»ªIDåˆ°ä¸­æ–‡åç§°çš„æ˜ å°„ï¼ˆä¸å®é™…æ˜ å°„ä¿æŒä¸€è‡´ï¼‰
        emotion_id_to_chinese = {
            0: 'ä¸­æ€§',      # neutral, astonished
            1: 'é«˜å…´',      # relaxed, happy, grateful, positive-other
            2: 'æ‚²ä¼¤',      # depress, fear, negative-other, sadness, worried
            3: 'æ„¤æ€’'       # anger, disgust
        }
        
        # ç»Ÿè®¡å„æ•°æ®é›†çš„æƒ…ç»ªåˆ†å¸ƒ
        train_emotions = []
        dev_emotions = []
        test_emotions = []
        
        # è®­ç»ƒé›†æƒ…ç»ªç»Ÿè®¡
        for vid in trainVids:
            if vid in video_labels:
                labels = video_labels[vid]
                train_emotions.extend(labels)
        
        # éªŒè¯é›†æƒ…ç»ªç»Ÿè®¡
        for vid in dev_vids:
            if vid in video_labels:
                labels = video_labels[vid]
                dev_emotions.extend(labels)
        
        # æµ‹è¯•é›†æƒ…ç»ªç»Ÿè®¡
        for vid in test_vids:
            if vid in video_labels:
                labels = video_labels[vid]
                test_emotions.extend(labels)
        
        # è®¡ç®—åˆ†å¸ƒ
        train_dist = Counter(train_emotions)
        dev_dist = Counter(dev_emotions)
        test_dist = Counter(test_emotions)
        
        # æ€»åˆ†å¸ƒ
        all_emotions = train_emotions + dev_emotions + test_emotions
        total_dist = Counter(all_emotions)
        
        print(f"\næ€»è¯è¯­æ•°: {len(all_emotions)}")
        print(f"è®­ç»ƒé›†è¯è¯­æ•°: {len(train_emotions)}")
        print(f"éªŒè¯é›†è¯è¯­æ•°: {len(dev_emotions)}")
        print(f"æµ‹è¯•é›†è¯è¯­æ•°: {len(test_emotions)}")
        
        print(f"\næ•°æ®é›†åˆ’åˆ†æ¯”ä¾‹:")
        print(f"è®­ç»ƒé›†: {len(train_emotions)/len(all_emotions)*100:.1f}%")
        print(f"éªŒè¯é›†: {len(dev_emotions)/len(all_emotions)*100:.1f}%")
        print(f"æµ‹è¯•é›†: {len(test_emotions)/len(all_emotions)*100:.1f}%")
        
        print(f"\næƒ…ç»ªåˆ†å¸ƒè¯¦æƒ…:")
        print(f"{'æƒ…ç»ª':<8} {'ä¸­æ–‡å':<6} {'è®­ç»ƒé›†':<10} {'éªŒè¯é›†':<10} {'æµ‹è¯•é›†':<10} {'æ€»è®¡':<8} {'æ¯”ä¾‹':<8}")
        print("-" * 70)
        
        for emotion_id in sorted(total_dist.keys()):
            emotion_chinese = emotion_id_to_chinese.get(emotion_id, f'æœªçŸ¥_{emotion_id}')
            
            train_count = train_dist.get(emotion_id, 0)
            dev_count = dev_dist.get(emotion_id, 0)
            test_count = test_dist.get(emotion_id, 0)
            total_count = total_dist.get(emotion_id, 0)
            percentage = total_count / len(all_emotions) * 100
            
            print(f"{emotion_id:<8} {emotion_chinese:<6} {train_count:<10} {dev_count:<10} {test_count:<10} {total_count:<8} {percentage:<7.1f}%")
        
        # è®¡ç®—æ¯ä¸ªæ•°æ®é›†å†…éƒ¨çš„æƒ…ç»ªåˆ†å¸ƒæ¯”ä¾‹
        print(f"\nå„æ•°æ®é›†å†…éƒ¨æƒ…ç»ªåˆ†å¸ƒæ¯”ä¾‹:")
        print(f"{'æƒ…ç»ª':<8} {'è®­ç»ƒé›†æ¯”ä¾‹':<12} {'éªŒè¯é›†æ¯”ä¾‹':<12} {'æµ‹è¯•é›†æ¯”ä¾‹':<12}")
        print("-" * 50)
        
        for emotion_id in sorted(total_dist.keys()):
            emotion_chinese = emotion_id_to_chinese.get(emotion_id, f'æœªçŸ¥_{emotion_id}')
            
            train_pct = train_dist.get(emotion_id, 0) / len(train_emotions) * 100 if len(train_emotions) > 0 else 0
            dev_pct = dev_dist.get(emotion_id, 0) / len(dev_emotions) * 100 if len(dev_emotions) > 0 else 0
            test_pct = test_dist.get(emotion_id, 0) / len(test_emotions) * 100 if len(test_emotions) > 0 else 0
            
            print(f"{emotion_id:<8} {train_pct:<11.1f}% {dev_pct:<11.1f}% {test_pct:<11.1f}%")
        
        # åˆ†ææ•°æ®é›†å¹³è¡¡æ€§
        print(f"\n=== æ•°æ®é›†å¹³è¡¡æ€§åˆ†æ ===")
        
        # è®¡ç®—æ¯ä¸ªæƒ…ç»ªçš„æ¯”ä¾‹
        emotion_ratios = {}
        for emotion_id, count in total_dist.items():
            emotion_ratios[emotion_id] = count / len(all_emotions)
        
        # æ‰¾å‡ºæœ€å¤šå’Œæœ€å°‘çš„æƒ…ç»ª
        max_emotion = max(emotion_ratios.items(), key=lambda x: x[1])
        min_emotion = min(emotion_ratios.items(), key=lambda x: x[1])
        
        print(f"æœ€å¤šæƒ…ç»ª: {emotion_id_to_chinese[max_emotion[0]]} ({max_emotion[1]*100:.1f}%)")
        print(f"æœ€å°‘æƒ…ç»ª: {emotion_id_to_chinese[min_emotion[0]]} ({min_emotion[1]*100:.1f}%)")
        print(f"ä¸å¹³è¡¡æ¯”ä¾‹: {max_emotion[1]/min_emotion[1]:.2f}:1")
        
        # è®¡ç®—åŸºå°¼ç³»æ•°ï¼ˆè¡¡é‡ä¸å¹³è¡¡ç¨‹åº¦ï¼‰
        sorted_ratios = sorted(emotion_ratios.values())
        n = len(sorted_ratios)
        gini = (2 * sum((i + 1) * ratio for i, ratio in enumerate(sorted_ratios))) / (n * sum(sorted_ratios)) - (n + 1) / n
        
        print(f"åŸºå°¼ç³»æ•°: {gini:.3f} (0=å®Œå…¨å¹³è¡¡, 1=å®Œå…¨ä¸å¹³è¡¡)")
        
        if gini < 0.2:
            print("âœ… æ•°æ®é›†ç›¸å¯¹å¹³è¡¡")
        elif gini < 0.4:
            print("âš ï¸ æ•°æ®é›†å­˜åœ¨ä¸€å®šä¸å¹³è¡¡")
        else:
            print("âŒ æ•°æ®é›†ä¸¥é‡ä¸å¹³è¡¡ï¼Œå»ºè®®è¿›è¡Œæ•°æ®å¢å¼º")
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self.create_emotion_distribution_chart(train_dist, dev_dist, test_dist, emotion_id_to_chinese)
        
        return {
            'train_dist': train_dist,
            'dev_dist': dev_dist, 
            'test_dist': test_dist,
            'total_dist': total_dist,
            'train_utterances': len(train_emotions),
            'dev_utterances': len(dev_emotions),
            'test_utterances': len(test_emotions),
            'total_utterances': len(all_emotions)
        }
    
    def create_emotion_distribution_chart(self, train_dist, dev_dist, test_dist, emotion_id_to_chinese):
        """åˆ›å»ºæƒ…ç»ªåˆ†å¸ƒå¯è§†åŒ–å›¾è¡¨"""
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns
            
            # è®¾ç½®ä¸­æ–‡å­—ä½“
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            
            # å‡†å¤‡æ•°æ®
            emotions = sorted(set(train_dist.keys()) | set(dev_dist.keys()) | set(test_dist.keys()))
            emotion_names = [emotion_id_to_chinese.get(e, f'æœªçŸ¥_{e}') for e in emotions]
            
            train_counts = [train_dist.get(e, 0) for e in emotions]
            dev_counts = [dev_dist.get(e, 0) for e in emotions]
            test_counts = [test_dist.get(e, 0) for e in emotions]
            
            # åˆ›å»ºå›¾è¡¨
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('æƒ…ç»ªåˆ†å¸ƒåˆ†æ', fontsize=16, fontweight='bold')
            
            # 1. æ€»ä½“åˆ†å¸ƒé¥¼å›¾
            ax1 = axes[0, 0]
            total_counts = [train_dist.get(e, 0) + dev_dist.get(e, 0) + test_dist.get(e, 0) for e in emotions]
            colors = plt.cm.Set3(np.linspace(0, 1, len(emotions)))
            
            wedges, texts, autotexts = ax1.pie(total_counts, labels=emotion_names, autopct='%1.1f%%', 
                                              colors=colors, startangle=90)
            ax1.set_title('æ€»ä½“æƒ…ç»ªåˆ†å¸ƒ')
            
            # 2. å„æ•°æ®é›†åˆ†å¸ƒå¯¹æ¯”æŸ±çŠ¶å›¾
            ax2 = axes[0, 1]
            x = np.arange(len(emotions))
            width = 0.25
            
            ax2.bar(x - width, train_counts, width, label='è®­ç»ƒé›†', alpha=0.8)
            ax2.bar(x, dev_counts, width, label='éªŒè¯é›†', alpha=0.8)
            ax2.bar(x + width, test_counts, width, label='æµ‹è¯•é›†', alpha=0.8)
            
            ax2.set_xlabel('æƒ…ç»ªç±»åˆ«')
            ax2.set_ylabel('è¯è¯­æ•°é‡')
            ax2.set_title('å„æ•°æ®é›†æƒ…ç»ªåˆ†å¸ƒå¯¹æ¯”')
            ax2.set_xticks(x)
            ax2.set_xticklabels(emotion_names, rotation=45)
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            
            # 3. è®­ç»ƒé›†åˆ†å¸ƒ
            ax3 = axes[1, 0]
            ax3.bar(emotion_names, train_counts, color='skyblue', alpha=0.7)
            ax3.set_title('è®­ç»ƒé›†æƒ…ç»ªåˆ†å¸ƒ')
            ax3.set_ylabel('è¯è¯­æ•°é‡')
            ax3.tick_params(axis='x', rotation=45)
            ax3.grid(True, alpha=0.3)
            
            # 4. æµ‹è¯•é›†åˆ†å¸ƒ
            ax4 = axes[1, 1]
            ax4.bar(emotion_names, test_counts, color='lightcoral', alpha=0.7)
            ax4.set_title('æµ‹è¯•é›†æƒ…ç»ªåˆ†å¸ƒ')
            ax4.set_ylabel('è¯è¯­æ•°é‡')
            ax4.tick_params(axis='x', rotation=45)
            ax4.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            output_file = 'emotion_distribution_analysis.png'
            plt.savefig(output_file, dpi=300, bbox_inches='tight')
            print(f"\nğŸ“Š æƒ…ç»ªåˆ†å¸ƒå›¾è¡¨å·²ä¿å­˜: {output_file}")
            
            # æ˜¾ç¤ºå›¾è¡¨
            plt.show()
            
        except ImportError:
            print("âš ï¸ matplotlibæœªå®‰è£…ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
        except Exception as e:
            print(f"âš ï¸ å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
    
    def process_cped_data(self, input_file, output_file):
        """å¤„ç†CPEDæ•°æ®çš„ä¸»å‡½æ•°"""
        print("å¼€å§‹GPUåŠ é€Ÿå¤„ç†CPEDæ•°æ®...")
        print("æ³¨æ„: ä½¿ç”¨GPUåŠ é€Ÿï¼Œå¤„ç†é€Ÿåº¦å¤§å¹…æå‡")
        
        # 1. åŠ è½½æ•°æ®
        df = self.load_cped_data(input_file)
        
        # 2. æŒ‰å¯¹è¯åˆ†ç»„
        conversations = self.group_by_conversation(df)
        
        # 3. å¤„ç†æ¯ä¸ªå¯¹è¯
        processed_conversations = {}
        for conv_id, utterances in tqdm(conversations.items(), desc="å¤„ç†å¯¹è¯"):
            processed_conv = self.process_single_conversation(conv_id, utterances)
            if processed_conv:
                processed_conversations[conv_id] = processed_conv
        
        # 4. åˆ†ææ•°æ®åˆ†å¸ƒ
        self.analyze_data_distribution(processed_conversations)
        
        # 5. è½¬æ¢ä¸ºIEMOCAPæ ¼å¼
        data = self.convert_to_iemocap_format(processed_conversations, output_file)
        
        print("CPEDæ•°æ®å¤„ç†å®Œæˆï¼")
        return data

    def augment_class(self, video_ids, video_text_dict, video_label_dict, video_speakers_dict, target_label, ratio=0.5):
        """
        å¯¹æŒ‡å®šç±»åˆ«åšç®€å•æ•°æ®å¢å¼ºï¼šå¤åˆ¶+é¡ºåºæ‰“ä¹±
        target_label: ç›®æ ‡ç±»åˆ«ï¼ˆå¦‚0=neutral, 2=sad, 3=angryï¼‰
        ratio: å¢å¼ºæ¯”ä¾‹
        """
        import random
        from copy import deepcopy
        new_vids = []
        for vid in video_ids:
            labels = video_label_dict.get(vid, [])
            speakers = video_speakers_dict.get(vid, [])
            # åªå¢å¼ºé•¿åº¦å¤§äº0ä¸”å­—æ®µé•¿åº¦ä¸€è‡´çš„æ ·æœ¬
            if not labels or not speakers:
                continue
            if not (len(labels) == len(speakers) == len(video_text_dict.get(vid, []))):
                continue
            if all(l == target_label for l in labels):
                if random.random() < ratio:
                    new_vid = vid + f'_aug{target_label}'
                    new_text = deepcopy(video_text_dict[vid])
                    new_labels = [target_label for _ in new_text]
                    new_speakers = deepcopy(video_speakers_dict[vid])
                    # ä¿è¯æ‰“ä¹±åä¸‰è€…é•¿åº¦ä¸€è‡´
                    zipped = list(zip(new_text, new_labels, new_speakers))
                    random.shuffle(zipped)
                    new_text, new_labels, new_speakers = zip(*zipped)
                    video_text_dict[new_vid] = list(new_text)
                    video_label_dict[new_vid] = list(new_labels)
                    video_speakers_dict[new_vid] = list(new_speakers)
                    new_vids.append(new_vid)
        video_ids.extend(new_vids)
        return video_ids, video_text_dict, video_label_dict, video_speakers_dict
    
def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='GPUåŠ é€ŸCPEDæ•°æ®å¤„ç†å™¨')
    parser.add_argument('--input', type=str, required=True, help='è¾“å…¥CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, required=True, help='è¾“å‡ºPKLæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--model', type=str, default='hfl/chinese-roberta-wwm-ext', help='RoBERTaæ¨¡å‹åç§°')
    parser.add_argument('--batch_size', type=int, default=64, help='æ‰¹å¤„ç†å¤§å°')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = os.path.dirname(args.output)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = GPUProcessor(model_name=args.model, batch_size=args.batch_size)
    
    # å¤„ç†æ•°æ®
    data = processor.process_cped_data(args.input, args.output)
    
    print(f"æ•°æ®å¤„ç†å®Œæˆï¼è¾“å‡ºæ–‡ä»¶: {args.output}")

if __name__ == "__main__":
    main()
